{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import GPT2Model\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2CIFAR10(nn.Module):\n",
    "    def __init__(self, patch_size=4, num_classes=10, freeze_gpt2=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained GPT2\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.hidden_size = self.gpt2.config.hidden_size  # 768 for base GPT2\n",
    "        \n",
    "        # CIFAR-10 characteristics\n",
    "        self.image_size = 32\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (self.image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding layer: from image patches to GPT2 hidden size\n",
    "        self.patch_embedding = nn.Conv2d(3, self.hidden_size, \n",
    "                                       kernel_size=patch_size, \n",
    "                                       stride=patch_size)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "        if freeze_gpt2:\n",
    "            # Freeze GPT2 parameters except LayerNorm and positional embeddings\n",
    "            for name, param in self.gpt2.named_parameters():\n",
    "                if 'ln' in name or 'wpe' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Convert image to patches\n",
    "        # Shape: (batch_size, hidden_size, h', w')\n",
    "        patches = self.patch_embedding(x)\n",
    "        \n",
    "        # Reshape and transpose for GPT2\n",
    "        # Shape: (batch_size, num_patches, hidden_size)\n",
    "        patches = rearrange(patches, 'b d h w -> b (h w) d')\n",
    "        \n",
    "        # Pass through GPT2 and get last hidden state\n",
    "        outputs = self.gpt2(inputs_embeds=patches)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Use the last token's representation for classification\n",
    "        cls_representation = hidden_states[:, -1]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(cls_representation)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Visualizer:\n",
    "    def __init__(self, model, device, class_names):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Save reference to GPT2 attention\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Register hook to get attention weights\n",
    "        def attention_hook(module, input, output):\n",
    "            # Get attention weights from output tuple\n",
    "            # Shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
    "            self.attention_maps.append(output[0].detach())\n",
    "        \n",
    "        # Register hooks for all attention blocks\n",
    "        for name, module in model.named_modules():\n",
    "            if \"attn\" in name and \"block\" in name:\n",
    "                module.register_forward_hook(attention_hook)\n",
    "        \n",
    "        # Standard CIFAR-10 normalization\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                               (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "    \n",
    "    def predict_and_visualize(self, images, true_labels=None, num_images=5):\n",
    "        \"\"\"\n",
    "        Visualize predictions and attention maps for a batch of images\n",
    "        \n",
    "        Args:\n",
    "            images: List of PIL images or tensor of shape (N, C, H, W)\n",
    "            true_labels: Optional list of true labels\n",
    "            num_images: Number of images to visualize\n",
    "        \"\"\"\n",
    "        # Clear previous attention maps\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Prepare images if they're PIL\n",
    "        if not torch.is_tensor(images):\n",
    "            tensors = []\n",
    "            for img in images:\n",
    "                tensors.append(self.transform(img))\n",
    "            images = torch.stack(tensors)\n",
    "        \n",
    "        # Move to device\n",
    "        images = images.to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(images[:num_images])\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "        \n",
    "        # Get attention weights (average over heads and layers)\n",
    "        # Shape: (batch_size, num_patches, num_patches)\n",
    "        avg_attention = torch.mean(torch.stack([\n",
    "            torch.mean(attention, dim=1) \n",
    "            for attention in self.attention_maps\n",
    "        ]), dim=0)\n",
    "        \n",
    "        # Create figure\n",
    "        num_cols = 3  # image, attention, patch attention\n",
    "        fig = plt.figure(figsize=(15, 5 * num_images))\n",
    "        \n",
    "        for idx in range(num_images):\n",
    "            # Original image with prediction\n",
    "            ax1 = plt.subplot(num_images, num_cols, idx * num_cols + 1)\n",
    "            img = images[idx].cpu()\n",
    "            img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + \\\n",
    "                  torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "            plt.imshow(img.permute(1, 2, 0).clip(0, 1))\n",
    "            \n",
    "            # Set title color based on prediction\n",
    "            pred_class = self.class_names[predictions[idx]]\n",
    "            if true_labels is not None:\n",
    "                color = 'green' if predictions[idx] == true_labels[idx] else 'red'\n",
    "                title = f'Pred: {pred_class}\\nTrue: {self.class_names[true_labels[idx]]}'\n",
    "            else:\n",
    "                color = 'black'\n",
    "                title = f'Pred: {pred_class}'\n",
    "            \n",
    "            ax1.set_title(title, color=color)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Attention heatmap\n",
    "            ax2 = plt.subplot(num_images, num_cols, idx * num_cols + 2)\n",
    "            attention_map = avg_attention[idx].cpu()\n",
    "            sns.heatmap(attention_map, cmap='viridis')\n",
    "            ax2.set_title('Average Self-Attention')\n",
    "            \n",
    "            # Patch-wise attention visualization\n",
    "            ax3 = plt.subplot(num_images, num_cols, idx * num_cols + 3)\n",
    "            # Get attention for the classification token (last token)\n",
    "            patch_attention = attention_map[-1, :-1].reshape(4, 4)  # for 8x8 patches\n",
    "            sns.heatmap(patch_attention, cmap='viridis')\n",
    "            ax3.set_title('Patch Attention Weights')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_7760\\427903016.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('C:\\\\Users\\\\Windows\\\\Documents\\\\CVC\\\\repos\\seeing-language\\\\notebooks\\wandb\\\\run-20241111_230849-kjps7qnm\\\\files\\\\best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Windows\\Documents\\CVC\\repos\\seeing-language\\notebooks\\02_visualizing_trained_gpt2cifar10.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataiter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Visualize predictions and attention\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m fig \u001b[39m=\u001b[39m visualizer\u001b[39m.\u001b[39;49mpredict_and_visualize(images[:\u001b[39m5\u001b[39;49m], labels[:\u001b[39m5\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# To save the figure\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# fig.savefig('predictions_attention.png', bbox_inches='tight', dpi=300)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Windows\\Documents\\CVC\\repos\\seeing-language\\notebooks\\02_visualizing_trained_gpt2cifar10.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     predictions \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Get attention weights (average over heads and layers)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Shape: (batch_size, num_patches, num_patches)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m avg_attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39;49mstack([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mmean(attention, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m attention \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_maps\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m ]), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Create figure\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Windows/Documents/CVC/repos/seeing-language/notebooks/02_visualizing_trained_gpt2cifar10.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m num_cols \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m  \u001b[39m# image, attention, patch attention\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('C:\\\\Users\\\\Windows\\\\Documents\\\\CVC\\\\repos\\seeing-language\\\\notebooks\\wandb\\\\run-20241111_230849-kjps7qnm\\\\files\\\\best_model.pth')\n",
    "model = GPT2CIFAR10()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = GPT2Visualizer(model=model, device='cpu', class_names=class_names)\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR10\n",
    "valset = datasets.CIFAR10(root='./data', train=False,\n",
    "                         download=True, transform=transform_val)\n",
    "\n",
    "valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get some test images\n",
    "dataiter = iter(valloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Visualize predictions and attention\n",
    "fig = visualizer.predict_and_visualize(images[:5], labels[:5])\n",
    "plt.show()\n",
    "\n",
    "# To save the figure\n",
    "# fig.savefig('predictions_attention.png', bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
